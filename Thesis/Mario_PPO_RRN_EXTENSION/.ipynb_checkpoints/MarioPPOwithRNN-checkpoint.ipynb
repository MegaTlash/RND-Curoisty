{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(frame):\n",
    "    frame = frame.sum(axis=-1)/765\n",
    "    frame = frame[20:210, :]\n",
    "    frame = frame[::2, ::2]\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./config.conf')\n",
    "\n",
    "\n",
    "#------------------------\n",
    "default = 'DEFAULT'\n",
    "#------------------------\n",
    "default_config = config[default]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_method = default_config['TrainMethod']\n",
    "max_step_per_episode = int(default_config['MaxStepPerEpisode'])\n",
    "\n",
    "class Environment(Process):\n",
    "    @abstractmethod\n",
    "    def run(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def pre_proc(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_init_state(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "def unwrap(env):\n",
    "    if hasattr(env, \"unwrapped\"):\n",
    "        return env.unwrapped\n",
    "    elif hasattr(env, \"env\"):\n",
    "        return unwrap(env.env)\n",
    "    elif hasattr(env, \"leg_env\"):\n",
    "        return unwrap(env.leg_env)\n",
    "    else:\n",
    "        return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, is_render, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "        self.is_render = is_render\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if self.is_render:\n",
    "                self.env.render()\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioEnvironment(Process):\n",
    "    def __init__(\n",
    "            self,\n",
    "            env_id,\n",
    "            is_render,\n",
    "            env_idx,\n",
    "            child_conn,\n",
    "            history_size=4,\n",
    "            life_done=True,\n",
    "            h=84,\n",
    "            w=84, movement=COMPLEX_MOVEMENT, sticky_action=True,\n",
    "            p=0.25):\n",
    "        super(MarioEnvironment, self).__init__()\n",
    "        self.daemon = True\n",
    "        self.env = JoypadSpace(\n",
    "            gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\n",
    "\n",
    "        self.is_render = is_render\n",
    "        self.env_idx = env_idx\n",
    "        self.steps = 0\n",
    "        self.episode = 0\n",
    "        self.rall = 0\n",
    "        self.recent_rlist = deque(maxlen=100)\n",
    "        self.child_conn = child_conn\n",
    "\n",
    "        self.life_done = life_done\n",
    "        self.sticky_action = sticky_action\n",
    "        self.last_action = 0\n",
    "        self.p = p\n",
    "\n",
    "        self.history_size = history_size\n",
    "        self.history = np.zeros([history_size, h, w])\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def run(self):\n",
    "        super(MarioEnvironment, self).run()\n",
    "        while True:\n",
    "            action = self.child_conn.recv()\n",
    "            if self.is_render:\n",
    "                self.env.render()\n",
    "\n",
    "            # sticky action\n",
    "            if self.sticky_action:\n",
    "                if np.random.rand() <= self.p:\n",
    "                    action = self.last_action\n",
    "                self.last_action = action\n",
    "\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "\n",
    "            # when Mario loses life, changes the state to the terminal\n",
    "            # state.\n",
    "            if self.life_done:\n",
    "                if self.lives > info['life'] and info['life'] > 0:\n",
    "                    force_done = True\n",
    "                    self.lives = info['life']\n",
    "                else:\n",
    "                    force_done = done\n",
    "                    self.lives = info['life']\n",
    "            else:\n",
    "                force_done = done\n",
    "\n",
    "            # reward range -15 ~ 15\n",
    "            log_reward = reward / 15\n",
    "            self.rall += log_reward\n",
    "\n",
    "            r = log_reward\n",
    "\n",
    "            self.history[:3, :, :] = self.history[1:, :, :]\n",
    "            self.history[3, :, :] = self.pre_proc(obs)\n",
    "\n",
    "            self.steps += 1\n",
    "\n",
    "            if done:\n",
    "                self.recent_rlist.append(self.rall)\n",
    "                print(\n",
    "                    \"[Episode {}({})] Step: {}  Reward: {}  Recent Reward: {}  Stage: {} current x:{}   max x:{}\".format(\n",
    "                        self.episode,\n",
    "                        self.env_idx,\n",
    "                        self.steps,\n",
    "                        self.rall,\n",
    "                        np.mean(\n",
    "                            self.recent_rlist),\n",
    "                        info['stage'],\n",
    "                        info['x_pos'],\n",
    "                        self.max_pos))\n",
    "\n",
    "                self.history = self.reset()\n",
    "\n",
    "            self.child_conn.send([self.history[:, :, :], r, force_done, done, log_reward])\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_action = 0\n",
    "        self.steps = 0\n",
    "        self.episode += 1\n",
    "        self.rall = 0\n",
    "        self.lives = 3\n",
    "        self.stage = 1\n",
    "        self.max_pos = 0\n",
    "        self.get_init_state(self.env.reset())\n",
    "        return self.history[:, :, :]\n",
    "\n",
    "    def pre_proc(self, X):\n",
    "        # grayscaling\n",
    "        x = cv2.cvtColor(X, cv2.COLOR_RGB2GRAY)\n",
    "        # resize\n",
    "        x = cv2.resize(x, (self.h, self.w))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_init_state(self, s):\n",
    "        for i in range(self.history_size):\n",
    "            self.history[i, :, :] = self.pre_proc(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PPOModel, self).__init__()\n",
    "        \n",
    "        linear = nn.Linear\n",
    "        \n",
    "        #Shared network (CNN Part)\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=4,\n",
    "                out_channels=32,\n",
    "                kernel_size=8,\n",
    "                stride=4\n",
    "            ),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2\n",
    "            ),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1\n",
    "            ),\n",
    "            nn.ELU(),\n",
    "            Flatten(),\n",
    "            linear(7 * 7 * 64, 256),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            linear(448, 448),\n",
    "            nn.ELU(),\n",
    "            linear(448, output_size)\n",
    "        )\n",
    "        \n",
    "        # The layer before having 2 value head\n",
    "        self.common_critic_layer = nn.Sequential(\n",
    "            linear(448, 448),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.critic_ext = linear(448, 1)\n",
    "        self.critic_int = linear(448, 1)\n",
    "        \n",
    "        \n",
    "        #Initialize the weights\n",
    "        for p in self.modules():\n",
    "            #Need to initialize the weights because it will return an error saying ELU does not have weights\n",
    "            if isinstance(p, nn.Conv2d):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "            \n",
    "            if isinstance(p, nn.Linear):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "                \n",
    "        \n",
    "        #Initialize critics\n",
    "        init.orthogonal_(self.critic_ext.weight, 0.01)\n",
    "        self.critic_ext.bias.data.zero_()\n",
    "        \n",
    "        init.orthogonal_(self.critic_int.weight, 0.01)\n",
    "        self.critic_int.bias.data.zero_()\n",
    "        \n",
    "        \n",
    "        #Intialize actor\n",
    "        for i in range(len(self.actor)):\n",
    "            if type(self.actor[i]) == nn.Linear:\n",
    "                init.orthogonal_(self.actor[i].weight, 0.01)\n",
    "                self.actor[i].bias.data.zero_()\n",
    "        \n",
    "        #Init value common layer\n",
    "        for i in range(len(self.common_critic_layer)):\n",
    "            if type(self.common_critic_layer[i]) == nn.Linear:\n",
    "                init.orthogonal_(self.common_critic_layer[i].weight, 0.1)\n",
    "                self.common_critic_layer[i].bias.data.zero_()\n",
    "                \n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.feature(state)\n",
    "        policy = self.actor(x)\n",
    "        value_ext = self.critic_ext(self.common_critic_layer(x) + x)\n",
    "        value_int = self.critic_int(self.common_critic_layer(x) + x)\n",
    "        return policy, value_ext, value_int\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RND Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNDModel(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(RNDModel, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        \n",
    "        feature_output = 7 * 7 * 64\n",
    "        \n",
    "        \n",
    "        #Prediction network\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels = 32,\n",
    "                kernel_size = 8,\n",
    "                stride=4\n",
    "            ),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2\n",
    "            ),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels = 64,\n",
    "                out_channels = 64,\n",
    "                kernel_size =3,\n",
    "                stride=1,\n",
    "            ),\n",
    "            nn.ELU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(feature_output, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 512)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #Taregt network\n",
    "        self.target = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels = 32,\n",
    "                kernel_size = 8,\n",
    "                stride=4\n",
    "            ),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2\n",
    "            ),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels = 64,\n",
    "                out_channels = 64,\n",
    "                kernel_size =3,\n",
    "                stride=1,\n",
    "            ),\n",
    "            nn.ELU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(feature_output, 512),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #Initalze the weights and biases\n",
    "        for p in self.modules():\n",
    "            \n",
    "            if isinstance(p, nn.Conv2d):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "            \n",
    "            if isinstance(p, nn.Linear):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "                \n",
    "        \n",
    "        #Set that target netowrk is not trainable\n",
    "        for param in self.target.parameters():\n",
    "            param.requres_grad = False\n",
    "    \n",
    "    \n",
    "    def forward(self, next_obs):\n",
    "        target_feature = self.target(next_obs)\n",
    "        predict_feature = self.predictor(next_obs)\n",
    "        \n",
    "        return predict_feature, target_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RND Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNDAgent(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        num_env,\n",
    "        num_step,\n",
    "        gamma,\n",
    "        lam=0.95,\n",
    "        learning_rate=1e-4,\n",
    "        ent_coef=0.01,\n",
    "        clip_grad_norm=0.5,\n",
    "        epoch=3,\n",
    "        batch_size=128,\n",
    "        ppo_eps=0.1,\n",
    "        update_proportion=0.25,\n",
    "        use_gae=True,\n",
    "        use_cuda=False):\n",
    "        \n",
    "        #Creating a PPO Model\n",
    "        self.model  =  PPOModel(input_size, output_size)\n",
    "        \n",
    "        self.num_env = num_env\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        self.num_step = num_step\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.learning_rate = learning_rate\n",
    "        self.ent_coef = ent_coef\n",
    "        self.clip_grad_norm = clip_grad_norm\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.ppo_eps = ppo_eps\n",
    "        self.update_proportion = update_proportion\n",
    "        self.use_gae = use_gae\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        print(\"DEVICE: \", self.device)\n",
    "        \n",
    "        \n",
    "        #Creating a RND Model\n",
    "        self.rnd = RNDModel(input_size, output_size)\n",
    "        \n",
    "        \n",
    "        #Using an optimizer (Adam)\n",
    "        self.optimizer = optim.Adam(list(self.model.parameters()) + list(self.rnd.predictor.parameters()), lr=learning_rate)\n",
    "        \n",
    "        \n",
    "        self.rnd = self.rnd.to(self.device)\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "    \n",
    "    def GetAction(self, state):\n",
    "        \n",
    "        #Transform the state into a float 32 tensor\n",
    "        state = torch.Tensor(state).to(self.device)\n",
    "        state = state.float()\n",
    "        \n",
    "        \n",
    "        #Getting the policy, value_ext, value _int\n",
    "        policy, value_ext, value_int = self.model(state)\n",
    "        \n",
    "        #Get action probability distrubiton\n",
    "        action_prob = F.softmax(policy, dim=-1).data.cpu().numpy()\n",
    "        \n",
    "        #select action\n",
    "        action = self.RandomChoiceProbIndex(action_prob)\n",
    "        \n",
    "        return action, value_ext.data.cpu().numpy().squeeze(), value_int.data.cput().numpy().squeeze()\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def RandomChoiceProbIndex(p, axis=1):\n",
    "        r = np.expand_dims(np.random.rand(p.shape[1 - axis]), axis=axis)\n",
    "        return (p.cumsum(axis=axis) > r).argmax(axis=axis)\n",
    "    \n",
    "    #Calculate Intrinsic reward\n",
    "    def ComputeIntrinisicReward(self, next_obs):\n",
    "        next_obs = torch.FloatTensor(next_obs).to(self.device)\n",
    "        \n",
    "        \n",
    "        #Get target feature\n",
    "        target_next_feature = self.rnd.target(next_obs)\n",
    "        \n",
    "        #Get prediction feature\n",
    "        predict_next_feature = self.rnd.predictor(next_obs)\n",
    "        \n",
    "        #Calculate intrinisc reward\n",
    "        intrinsic_reward = (target_next_feature - predict_next_feature).pow(2).sum(1) / 2\n",
    "        \n",
    "        return intrinisc_rward.data.cpu().num()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_model(self, s_batch, target_ext_batch, target_int_batch, y_batch, adv_batch, next_obs_batch, old_policy):\n",
    "        s_batch = torch.FloatTensor(s_batch).to(self.device)\n",
    "        target_ext_batch = torch.FloatTensor(target_ext_batch).to(self.device)\n",
    "        target_int_batch = torch.FloatTensor(target_int_batch).to(self.device)\n",
    "        y_batch = torch.LongTensor(y_batch).to(self.device)\n",
    "        adv_batch = torch.FloatTensor(adv_batch).to(self.device)\n",
    "        next_obs_batch = torch.FloatTensor(next_obs_batch).to(self.device)\n",
    "        \n",
    "        sample_range = np.arrange(len(s_batch))\n",
    "        forward_mse = nn.MSELoss(reduction = \"none\")\n",
    "        \n",
    "        \n",
    "        #Getting old policy\n",
    "        with torch.no_grad():\n",
    "            policy_old_list = torch.stack(old_policy).permute(1,0,2).contiguous().view(-1, self.output_size).to(self.device)\n",
    "\n",
    "            m_old = Categorical(F.solftmax(policy_old_list, dim=-1))\n",
    "            log_prob_old = m_old.log_prob(y_batch)\n",
    "            \n",
    "        for i in range(self.epoch):\n",
    "            #Doing minibatches of training\n",
    "            np.random.shuffle(sample_range)\n",
    "            \n",
    "            for j in range(int(len(s_batch)/ self.batch_size)):\n",
    "                sample_idx = sample_range[self.batch_size * j:self.batch_size * (j + 1)]\n",
    "                \n",
    "                \n",
    "                #--------------------------------------------------------------------------------------\n",
    "                #Curiosity driven calcuation (RND)\n",
    "                predict_next_state_feature, target_next_state_feature = self.rnd(next_obs_batch[sample_idx])\n",
    "                \n",
    "                \n",
    "                forward_loss = forward_mse(predict_next_state_feature, target_next_state_feature.detach()).mean(-1)\n",
    "                mask = torch.rand(len(forward_loss)).to(self.device)\n",
    "                mask = (mask < self.update_proportion).type(torch.FloatTensor).to(self.device)\n",
    "                forward_loss = (forward_loss * mask).sum() / torch.max(mask.sum(), torch.Tensor([1]).to(self.device))\n",
    "                \n",
    "                #--------------------------------------------------------------------------------------------------\n",
    "                \n",
    "                policy, value_ext, value_int = self.model(s_batch[sample_idx])\n",
    "                m = Categorical(F.softmax(policy, dim=-1))\n",
    "                log_prob = m.log_prob(y_batch[sample_idx])\n",
    "                \n",
    "                \n",
    "                ratio = torch.exp(log_prob - log_prob_old[sample_idx])\n",
    "                \n",
    "                surr1 = ratio * adv_batch[sample_idx]\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.ppo_eps, 1.0 + self.ppo_eps) * adv_batch[sample_idx]\n",
    "                \n",
    "                #Calculate actor loss\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                #Calcualate critic loss\n",
    "                critic_ext_loss = F.mse_loss(value_ext.sum(1), target_ext_batch[sample_idx])\n",
    "                critic_int_loss = F.mse_loss(value_int.sum(1), target_int_batch[sample_idx])\n",
    "                \n",
    "                \n",
    "                #Critic loss = critic E loss + critic I loss\n",
    "                critic_loss = critic_ext_loss + critic_int_loss\n",
    "                \n",
    "                #Calculate the entropy\n",
    "                # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.\n",
    "                entropy = m.entropy().mean()\n",
    "                \n",
    "                #Reset the gardients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "                #Calculate the loss\n",
    "                #Total loss = Policy gradient loss - entropy * entropy coefficent + Value coefficent * value loss + foward_loss\n",
    "                loss  = actor_loss + 0.5 * critic_loss - self.ent_coef * entropy + forward_loss\n",
    "                \n",
    "                \n",
    "                #Backpropagation\n",
    "                loss.backward()\n",
    "                global_grad_norm_(list(self.model.parameters())+list(self.rnd.predictor.parameters()))\n",
    "                self.optimizer.step()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._six import inf\n",
    "\n",
    "use_gae = default_config.getboolean('UseGAE')\n",
    "lam = float(default_config['Lambda'])\n",
    "\n",
    "def make_train_data(reward, done, value, gamma, num_step, num_worker):\n",
    "    discounted_return = np.empty([num_worker, num_step])\n",
    "    \n",
    "    \n",
    "     # Discounted Return\n",
    "    if use_gae:\n",
    "        gae = np.zeros_like([num_worker, ])\n",
    "        for t in range(num_step - 1, -1, -1):\n",
    "            delta = reward[:, t] + gamma * value[:, t + 1] * (1 - done[:, t]) - value[:, t]\n",
    "            gae = delta + gamma * lam * (1 - done[:, t]) * gae\n",
    "\n",
    "            discounted_return[:, t] = gae + value[:, t]\n",
    "\n",
    "            # For Actor\n",
    "        adv = discounted_return - value[:, :-1]\n",
    "\n",
    "    else:\n",
    "        running_add = value[:, -1]\n",
    "        for t in range(num_step - 1, -1, -1):\n",
    "            running_add = reward[:, t] + gamma * running_add * (1 - done[:, t])\n",
    "            discounted_return[:, t] = running_add\n",
    "\n",
    "        # For Actor\n",
    "        adv = discounted_return - value[:, :-1]\n",
    "\n",
    "    return discounted_return.reshape([-1]), adv.reshape([-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMeanStd(object):\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        delta = batch_mean - self.mean\n",
    "        tot_count = self.count + batch_count\n",
    "\n",
    "        new_mean = self.mean + delta * batch_count / tot_count\n",
    "        m_a = self.var * (self.count)\n",
    "        m_b = batch_var * (batch_count)\n",
    "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n",
    "        new_var = M2 / (self.count + batch_count)\n",
    "\n",
    "        new_count = batch_count + self.count\n",
    "\n",
    "        self.mean = new_mean\n",
    "        self.var = new_var\n",
    "        self.count = new_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardForwardFilter(object):\n",
    "    def __init__(self, gamma):\n",
    "        self.rewems = None\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, rews):\n",
    "        if self.rewems is None:\n",
    "            self.rewems = rews\n",
    "        else:\n",
    "            self.rewems = self.rewems * self.gamma + rews\n",
    "        return self.rewems\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis]  # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis]  # dito\n",
    "    return e_x / div\n",
    "\n",
    "\n",
    "def global_grad_norm_(parameters, norm_type=2):\n",
    "    r\"\"\"Clips gradient norm of an iterable of parameters.\n",
    "    The norm is computed over all gradients together, as if they were\n",
    "    concatenated into a single vector. Gradients are modified in-place.\n",
    "    Arguments:\n",
    "        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n",
    "            single Tensor that will have gradients normalized\n",
    "        max_norm (float or int): max norm of the gradients\n",
    "        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n",
    "            infinity norm.\n",
    "    Returns:\n",
    "        Total norm of the parameters (viewed as a single vector).\n",
    "    \"\"\"\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "    norm_type = float(norm_type)\n",
    "    if norm_type == inf:\n",
    "        total_norm = max(p.grad.data.abs().max() for p in parameters)\n",
    "    else:\n",
    "        total_norm = 0\n",
    "        for p in parameters:\n",
    "            param_norm = p.grad.data.norm(norm_type)\n",
    "            total_norm += param_norm.item() ** norm_type\n",
    "        total_norm = total_norm ** (1. / norm_type)\n",
    "\n",
    "    return total_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.multiprocessing import Pipe\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    #Printing out the config hyperparameters\n",
    "    print({section: dict(config[section]) for section in config.sections()})\n",
    "    \n",
    "    #Select the training environment\n",
    "    env_id = default_config['EnvID']\n",
    "    \n",
    "    #Select the env_type\n",
    "    env_type = default_config['EnvType']\n",
    "    \n",
    "    if env_type == 'mario':\n",
    "        env = JoypadSpace(gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\n",
    "    \n",
    "    #Getting the state size and action space\n",
    "    input_size = env.observation_space.shape\n",
    "    print(\"INPUT SIZE\", input_size)\n",
    "    output_size = env.action_space.n\n",
    "    print(\"ACTION SPACE\", output_size)\n",
    "    \n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    is_load_model = False\n",
    "    is_render = False\n",
    "    \n",
    "    \n",
    "    # Defining the model path names\n",
    "    model_path = 'models/{}.model'.format(env_id)\n",
    "    predictor_path = 'models/{}.pred'.format(env_id)\n",
    "    target_path = 'models/{}.target'.format(env_id)\n",
    "    \n",
    "    #Set the writer from Tensorboard\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    #Get the config hyperparameters\n",
    "    use_cuda = default_config.getboolean('UseGPU')\n",
    "    \n",
    "    #GAE hyperparameters\n",
    "    use_gae = default_config.getboolean('UseGAE')\n",
    "    lam = float(default_config['Lambda'])\n",
    "    \n",
    "    #Number of different environments to run in parallel\n",
    "    num_worker = int(default_config['NumEnv'])\n",
    "    \n",
    "    num_step = int(default_config['NumStep'])\n",
    "    \n",
    "    #PPO epsilon (aka what will help us to define the cliprange)\n",
    "    ppo_eps = float(default_config['PPOEps'])\n",
    "        \n",
    "    epoch = int(default_config['Epoch']) \n",
    "    \n",
    "    mini_batch = int(default_config['MiniBatch'])\n",
    "    \n",
    "    batch_size = int(num_step * num_worker / mini_batch)\n",
    "    \n",
    "    learning_rate = float(default_config['LearningRate'])\n",
    "    \n",
    "    entropy_coef = float(default_config['Entropy'])\n",
    "    \n",
    "    #Extrinsic reward discound rate\n",
    "    gamma = float(default_config['Gamma'])\n",
    "    \n",
    "    \n",
    "    #Intrinsic reward discound rate\n",
    "    int_gamma = float(default_config['IntGamma'])\n",
    "    \n",
    "    #Gradient normalization clip\n",
    "    clip_grad_norm = float(default_config['ClipGradNorm'])\n",
    "    \n",
    "    \n",
    "    #Extrinisic reward coefficent\n",
    "    ext_coef = float(default_config['ExtCoef'])\n",
    "    \n",
    "    #Intrinsic reward coefficent\n",
    "    int_coef = float(default_config['IntCoef'])\n",
    "    \n",
    "    #Use stick action\n",
    "    sticky_action = default_config.getboolean('StickyAction')\n",
    "    action_prob = float(default_config['ActionProb'])\n",
    "    \n",
    "    \n",
    "    life_done = default_config.getboolean('LifeDone')\n",
    "    \n",
    "    \n",
    "    reward_rms = RunningMeanStd()\n",
    "    obs_rms = RunningMeanStd(shape=(1, 1, 84, 84))\n",
    "    pre_obs_norm_step = int(default_config['ObsNormStep'])\n",
    "    discounted_reward = RewardForwardFilter(int_gamma)\n",
    "    \n",
    "    \n",
    "    #Instantiate the agent\n",
    "    agent = RNDAgent(\n",
    "        input_size,\n",
    "        output_size,\n",
    "        num_worker,\n",
    "        num_step,\n",
    "        gamma,\n",
    "        lam=lam,\n",
    "        learning_rate=learning_rate,\n",
    "        ent_coef= entropy_coef,\n",
    "        clip_grad_norm = clip_grad_norm,\n",
    "        epoch=epoch,\n",
    "        batch_size = batch_size,\n",
    "        ppo_eps = ppo_eps,\n",
    "        use_cuda = use_cuda,\n",
    "        use_gae=use_gae\n",
    "        \n",
    "    )\n",
    "    \n",
    "    \n",
    "    #Mario Environment\n",
    "    if default_config['EnvType'] == 'mario':\n",
    "        env_type = MarioEnvironment\n",
    "    #Add a legend of zelda environment later\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # Loads models\n",
    "    if is_load_model:\n",
    "        if use_cuda:\n",
    "            print(\"Loading PPO Saved Model using GPU\")\n",
    "            agent.model.load_state_dict(torch.load(model_path))\n",
    "        else:\n",
    "            print(\"Loading PPO Saved Model using CPU\")\n",
    "            agent.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "\n",
    "    works = []\n",
    "    parent_conns = []\n",
    "    \n",
    "    # Generate the different environements\n",
    "    for idx in range(num_worker):\n",
    "        parent_conn, child_conn = Pipe()\n",
    "        work = env_type(env_id, is_render, idx, child_conn, sticky_action=sticky_action, p=action_prob,\n",
    "                        life_done=life_done)\n",
    "        work.start()\n",
    "        works.append(work)\n",
    "        parent_conn.append(parent_conn)\n",
    "        child_conn.append(child_conn)\n",
    "        \n",
    "    states = np.zeros([num_worker, 4, 84, 84])\n",
    "\n",
    "    sample_episode = 0\n",
    "    sample_rall = 0\n",
    "    sample_step = 0\n",
    "    sample_env_idx = 0\n",
    "    sample_i_rall = 0\n",
    "    global_update = 0\n",
    "    global_step = 0\n",
    "\n",
    "    # normalize obs\n",
    "    print('Start to initailize observation normalization parameter.....')\n",
    "    next_obs = []\n",
    "    for step in range(num_step * pre_obs_norm_step):\n",
    "        actions = np.random.randint(0, output_size, size=(num_worker,))\n",
    "\n",
    "        for parent_conn, action in zip(parent_conns, actions):\n",
    "            parent_conn.send(action)\n",
    "\n",
    "        for parent_conn in parent_conns:\n",
    "            s, r, d, rd, lr = parent_conn.recv()\n",
    "            next_obs.append(s[3, :, :].reshape([1, 84, 84]))\n",
    "\n",
    "        if len(next_obs) % (num_step * num_worker) == 0:\n",
    "            next_obs = np.stack(next_obs)\n",
    "            obs_rms.update(next_obs)\n",
    "            next_obs = []\n",
    "    print('End to initalize...')\n",
    "    while True:\n",
    "        total_state = []\n",
    "        total_reward = []\n",
    "        total_done = []\n",
    "        total_next_state = []\n",
    "        total_action = []\n",
    "        total_int_reward = []\n",
    "        total_next_state = []\n",
    "        total_next_obs = []\n",
    "        total_ext_values = []\n",
    "        total_int_values = []\n",
    "        total_policy = []\n",
    "        total_policy_np = []\n",
    "        \n",
    "        global_step += (num_worker * num_step)\n",
    "        global_update += 1\n",
    "\n",
    "        # Step 1. n-step rollout\n",
    "        for _ in range(num_step):\n",
    "            actions, value_ext, value_int, policy = agent.get_action(np.float32(states) / 255.)\n",
    "\n",
    "            for parent_conn, action in zip(parent_conns, actions):\n",
    "                parent_conn.send(action)\n",
    "\n",
    "            next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\n",
    "            for parent_conn in parent_conns:\n",
    "                s, r, d, rd, lr = parent_conn.recv()\n",
    "                next_states.append(s)\n",
    "                rewards.append(r)\n",
    "                dones.append(d)\n",
    "                real_dones.append(rd)\n",
    "                log_rewards.append(lr)\n",
    "                next_obs.append(s[3, :, :].reshape([1, 84, 84]))\n",
    "\n",
    "            next_states = np.stack(next_states)\n",
    "            rewards = np.hstack(rewards)\n",
    "            dones = np.hstack(dones)\n",
    "            real_dones = np.hstack(real_dones)\n",
    "            next_obs = np.stack(next_obs)\n",
    "\n",
    "            # total reward = int reward + ext Reward\n",
    "            intrinsic_reward = agent.compute_intrinsic_reward(\n",
    "                ((next_obs - obs_rms.mean) / np.sqrt(obs_rms.var)).clip(-5, 5))\n",
    "            intrinsic_reward = np.hstack(intrinsic_reward)\n",
    "            sample_i_rall += intrinsic_reward[sample_env_idx]\n",
    "\n",
    "            total_next_obs.append(next_obs)\n",
    "            total_int_reward.append(intrinsic_reward)\n",
    "            total_state.append(states)\n",
    "            total_reward.append(rewards)\n",
    "            total_done.append(dones)\n",
    "            total_action.append(actions)\n",
    "            total_ext_values.append(value_ext)\n",
    "            total_int_values.append(value_int)\n",
    "            total_policy.append(policy)\n",
    "            total_policy_np.append(policy.cpu().numpy())\n",
    "\n",
    "            states = next_states[:, :, :, :]\n",
    "\n",
    "            sample_rall += log_rewards[sample_env_idx]\n",
    "\n",
    "            sample_step += 1\n",
    "            if real_dones[sample_env_idx]:\n",
    "                sample_episode += 1\n",
    "                writer.add_scalar('data/reward_per_epi', sample_rall, sample_episode)\n",
    "                writer.add_scalar('data/reward_per_rollout', sample_rall, global_update)\n",
    "                writer.add_scalar('data/step', sample_step, sample_episode)\n",
    "                sample_rall = 0\n",
    "                sample_step = 0\n",
    "                sample_i_rall = 0\n",
    "\n",
    "        # calculate last next value\n",
    "        _, value_ext, value_int, _ = agent.get_action(np.float32(states) / 255.)\n",
    "        total_ext_values.append(value_ext)\n",
    "        total_int_values.append(value_int)\n",
    "        # --------------------------------------------------\n",
    "\n",
    "        total_state = np.stack(total_state).transpose([1, 0, 2, 3, 4]).reshape([-1, 4, 84, 84])\n",
    "        total_reward = np.stack(total_reward).transpose().clip(-1, 1)\n",
    "        total_action = np.stack(total_action).transpose().reshape([-1])\n",
    "        total_done = np.stack(total_done).transpose()\n",
    "        total_next_obs = np.stack(total_next_obs).transpose([1, 0, 2, 3, 4]).reshape([-1, 1, 84, 84])\n",
    "        total_ext_values = np.stack(total_ext_values).transpose()\n",
    "        total_int_values = np.stack(total_int_values).transpose()\n",
    "        total_logging_policy = np.vstack(total_policy_np)\n",
    "\n",
    "        # Step 2. calculate intrinsic reward\n",
    "        # running mean intrinsic reward\n",
    "        total_int_reward = np.stack(total_int_reward).transpose()\n",
    "        total_reward_per_env = np.array([discounted_reward.update(reward_per_step) for reward_per_step in\n",
    "                                         total_int_reward.T])\n",
    "        mean, std, count = np.mean(total_reward_per_env), np.std(total_reward_per_env), len(total_reward_per_env)\n",
    "        reward_rms.update_from_moments(mean, std ** 2, count)\n",
    "\n",
    "        # normalize intrinsic reward\n",
    "        total_int_reward /= np.sqrt(reward_rms.var)\n",
    "        writer.add_scalar('data/int_reward_per_epi', np.sum(total_int_reward) / num_worker, sample_episode)\n",
    "        writer.add_scalar('data/int_reward_per_rollout', np.sum(total_int_reward) / num_worker, global_update)\n",
    "        # -------------------------------------------------------------------------------------------\n",
    "\n",
    "        # logging Max action probability\n",
    "        writer.add_scalar('data/max_prob', softmax(total_logging_policy).max(1).mean(), sample_episode)\n",
    "\n",
    "        # Step 3. make target and advantage\n",
    "        # extrinsic reward calculate\n",
    "        ext_target, ext_adv = make_train_data(total_reward,\n",
    "                                              total_done,\n",
    "                                              total_ext_values,\n",
    "                                              gamma,\n",
    "                                              num_step,\n",
    "                                              num_worker)\n",
    "\n",
    "        # intrinsic reward calculate\n",
    "        # None Episodic\n",
    "        int_target, int_adv = make_train_data(total_int_reward,\n",
    "                                              np.zeros_like(total_int_reward),\n",
    "                                              total_int_values,\n",
    "                                              int_gamma,\n",
    "                                              num_step,\n",
    "                                              num_worker)\n",
    "\n",
    "        # add ext adv and int adv\n",
    "        total_adv = int_adv * int_coef + ext_adv * ext_coef\n",
    "        # -----------------------------------------------\n",
    "\n",
    "        # Step 4. update obs normalize param\n",
    "        obs_rms.update(total_next_obs)\n",
    "        # -----------------------------------------------\n",
    "\n",
    "        # Step 5. Training\n",
    "        agent.train_model(np.float32(total_state) / 255., ext_target, int_target, total_action,\n",
    "                          total_adv, ((total_next_obs - obs_rms.mean) / np.sqrt(obs_rms.var)).clip(-5, 5),\n",
    "                          total_policy)\n",
    "\n",
    "        if global_step % (num_worker * num_step * 100) == 0:\n",
    "            print(\"Num Step: \", num_step)\n",
    "            print('Now Global Step :{}'.format(global_step))\n",
    "            torch.save(agent.model.state_dict(), model_path)\n",
    "            torch.save(agent.rnd.predictor.state_dict(), predictor_path)\n",
    "            torch.save(agent.rnd.target.state_dict(), target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "INPUT SIZE (240, 256, 3)\n",
      "ACTION SPACE 12\n",
      "DEVICE:  cuda\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-c87fe2eed485>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    135\u001b[0m         work = env_type(env_id, is_render, idx, child_conn, sticky_action=sticky_action, p=action_prob,\n\u001b[0;32m    136\u001b[0m                         life_done=life_done)\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0mworks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Tamilesh\\Applications\\Anaconda\\Anaconda\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Tamilesh\\Applications\\Anaconda\\Anaconda\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Tamilesh\\Applications\\Anaconda\\Anaconda\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Tamilesh\\Applications\\Anaconda\\Anaconda\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Tamilesh\\Applications\\Anaconda\\Anaconda\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
